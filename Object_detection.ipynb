{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c13746b6bdbc4379a58e1f441c2d12ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6784e1e836349309748a91334f43ecd",
              "IPY_MODEL_d6862bbfcfd6490a9799f16bc2afbe24",
              "IPY_MODEL_40275897b41c40d38d7d142df3c6d68f"
            ],
            "layout": "IPY_MODEL_56afc3cc5e0b403b9519956db76636af"
          }
        },
        "b6784e1e836349309748a91334f43ecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f6901b86f645928b5b3da9f561b226",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_955047e15bc04bb6be43c3fa2cbabe5e",
            "value": "100%"
          }
        },
        "d6862bbfcfd6490a9799f16bc2afbe24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83d45783814b4bbf8ad5aa755d220be2",
            "max": 93622629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cf13f89761844eab2797258ebb98faa",
            "value": 93622629
          }
        },
        "40275897b41c40d38d7d142df3c6d68f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2327d5f3cef4e078a5d0ab57f1c00dd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4d19047d1b0a4b108b9ceeb4e8f49742",
            "value": " 89.3M/89.3M [00:01&lt;00:00, 66.8MB/s]"
          }
        },
        "56afc3cc5e0b403b9519956db76636af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7f6901b86f645928b5b3da9f561b226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "955047e15bc04bb6be43c3fa2cbabe5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83d45783814b4bbf8ad5aa755d220be2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cf13f89761844eab2797258ebb98faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2327d5f3cef4e078a5d0ab57f1c00dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d19047d1b0a4b108b9ceeb4e8f49742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# We have taken the model from Utlralytics github repository. We do not take any credit for the yolo v5 model\n",
        "#we have only used it as a pre-trained model and used for object detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "uhpsj-n-EnbI",
        "outputId": "38971d06-dff2-4702-b938-1e81911cfbbc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ce86fe1dacd6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Places: A 10 million Image Database for Scene Recognition\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R2z8ObPc6XB5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520,
          "referenced_widgets": [
            "c13746b6bdbc4379a58e1f441c2d12ca",
            "b6784e1e836349309748a91334f43ecd",
            "d6862bbfcfd6490a9799f16bc2afbe24",
            "40275897b41c40d38d7d142df3c6d68f",
            "56afc3cc5e0b403b9519956db76636af",
            "f7f6901b86f645928b5b3da9f561b226",
            "955047e15bc04bb6be43c3fa2cbabe5e",
            "83d45783814b4bbf8ad5aa755d220be2",
            "4cf13f89761844eab2797258ebb98faa",
            "f2327d5f3cef4e078a5d0ab57f1c00dd",
            "4d19047d1b0a4b108b9ceeb4e8f49742"
          ]
        },
        "outputId": "139ec405-7f43-4279-fde4-fb8c44bf1602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"gitpython>=3.1.30\" not found, attempting AutoUpdate...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gitpython>=3.1.30\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 184.3/184.3 KB 7.9 MB/s eta 0:00:00\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 62.7/62.7 KB 6.3 MB/s eta 0:00:00\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.31 smmap-5.0.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 üöÄ 2023-3-27 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt to yolov5l.pt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/89.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c13746b6bdbc4379a58e1f441c2d12ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Fusing layers... \n",
            "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5l')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
      ],
      "metadata": {
        "id": "E_BTPUVlYOHc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "y1jqfo_uYN9m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pickle\n",
        "import torchvision\n",
        "from torchvision.transforms import Resize, Normalize\n",
        "from torchvision.io import read_image\n",
        "import torchvision.transforms as T\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "P0DO08SwPj_l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = 'https://i.ytimg.com/vi/q71MCWAEfL8/maxresdefault.jpg'  # or file, Path, PIL, OpenCV, numpy, list\n",
        "results = model(img)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoYLcC6RDTnc",
        "outputId": "0f318e19-f642-447f-d2b9-7dce524da32e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image 1/1: 720x1280 17 persons, 1 car, 1 bus, 2 trucks, 6 traffic lights, 2 backpacks, 1 handbag\n",
            "Speed: 202.7ms pre-process, 35.0ms inference, 21.9ms NMS per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOLO V5 OBJ dectection**"
      ],
      "metadata": {
        "id": "aimkol3kFPnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_path = '/content/drive/MyDrive/IR Project/Dataset/flickr30k_images'\n",
        "mod_path = '/content/drive/MyDrive/IR Project/Dataset/Xtra ftr modules'"
      ],
      "metadata": {
        "id": "0fizkNP_Dk8x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = T.ToPILImage()\n",
        "def preproc_image(img):\n",
        "  img = img.to(device)\n",
        "  img = img.to(dtype = torch.float32)\n",
        "  img = transformer(img)\n",
        "  return img"
      ],
      "metadata": {
        "id": "d1oI_JASXE0h"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOCfjZ-zvQ9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_to_desc = {}\n",
        "idx = 0\n",
        "for doc in os.listdir(ds_path):\n",
        "  img_path = ds_path+f'/{doc}'\n",
        "  img = read_image(img_path)\n",
        "  \n",
        "  img = preproc_image(img)\n",
        "  res = model(img)\n",
        "  img_to_desc[doc] = str(res)\n",
        "  if (idx+1)%50 == 0:\n",
        "    with open(mod_path +'/img_to_objects.pkl', 'wb') as f:\n",
        "      pickle.dump(img_to_desc, f)\n",
        "    print(res)\n",
        "  idx+=1\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk6i1wmVKajv",
        "outputId": "c862e7c6-07df-4966-e266-5bfec907c8f8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image 1/1: 375x500 2 dogs, 1 cake\n",
            "Speed: 2.2ms pre-process, 22.5ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x375 4 persons, 1 bottle, 1 cup, 1 chair, 1 dining table\n",
            "Speed: 1.9ms pre-process, 21.0ms inference, 1.0ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 333x500 1 person, 1 snowboard\n",
            "Speed: 5.9ms pre-process, 64.5ms inference, 4.8ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 334x500 6 persons, 1 backpack\n",
            "Speed: 2.7ms pre-process, 23.0ms inference, 1.5ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 337x500 1 teddy bear\n",
            "Speed: 1.9ms pre-process, 24.4ms inference, 1.0ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 334x500 1 person, 1 snowboard, 1 skateboard\n",
            "Speed: 1.9ms pre-process, 24.2ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 334x500 (no detections)\n",
            "Speed: 2.3ms pre-process, 24.0ms inference, 0.7ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 boat\n",
            "Speed: 2.3ms pre-process, 25.6ms inference, 1.8ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 400x500 3 persons\n",
            "Speed: 2.4ms pre-process, 23.6ms inference, 1.2ms NMS per image at shape (1, 3, 512, 640)\n",
            "image 1/1: 500x334 2 persons\n",
            "Speed: 2.3ms pre-process, 23.0ms inference, 1.3ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 347x500 1 person, 1 snowboard\n",
            "Speed: 2.9ms pre-process, 24.1ms inference, 1.5ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 5 persons\n",
            "Speed: 2.8ms pre-process, 21.6ms inference, 1.5ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 345x500 (no detections)\n",
            "Speed: 2.0ms pre-process, 23.4ms inference, 1.0ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x375 1 person, 1 bicycle, 2 trucks\n",
            "Speed: 2.2ms pre-process, 22.2ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 375x500 3 persons, 1 frisbee\n",
            "Speed: 2.3ms pre-process, 24.8ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 (no detections)\n",
            "Speed: 2.1ms pre-process, 24.9ms inference, 0.7ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 3 persons\n",
            "Speed: 2.2ms pre-process, 24.5ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 362x480 1 truck, 1 dog\n",
            "Speed: 2.5ms pre-process, 25.7ms inference, 2.9ms NMS per image at shape (1, 3, 512, 640)\n",
            "image 1/1: 332x500 1 person, 1 truck\n",
            "Speed: 2.9ms pre-process, 24.2ms inference, 1.5ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 463x500 1 person, 1 frisbee, 1 snowboard\n",
            "Speed: 3.9ms pre-process, 28.2ms inference, 1.9ms NMS per image at shape (1, 3, 608, 640)\n",
            "image 1/1: 333x500 11 persons\n",
            "Speed: 2.0ms pre-process, 24.5ms inference, 1.5ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x426 1 person, 1 dog, 1 sheep\n",
            "Speed: 2.7ms pre-process, 29.6ms inference, 1.3ms NMS per image at shape (1, 3, 640, 576)\n",
            "image 1/1: 334x500 3 persons, 1 traffic light, 1 umbrella, 1 handbag\n",
            "Speed: 2.1ms pre-process, 24.0ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x333 7 persons, 1 car, 1 traffic light\n",
            "Speed: 1.9ms pre-process, 22.1ms inference, 1.6ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 335x500 18 persons, 1 kite\n",
            "Speed: 2.1ms pre-process, 22.9ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 378x500 2 persons, 1 teddy bear\n",
            "Speed: 2.1ms pre-process, 25.9ms inference, 1.1ms NMS per image at shape (1, 3, 512, 640)\n",
            "image 1/1: 500x333 3 persons\n",
            "Speed: 2.9ms pre-process, 21.1ms inference, 1.8ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x334 1 person\n",
            "Speed: 2.1ms pre-process, 21.8ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x311 1 person, 1 snowboard, 1 skateboard\n",
            "Speed: 2.0ms pre-process, 20.4ms inference, 1.1ms NMS per image at shape (1, 3, 640, 416)\n",
            "image 1/1: 500x375 1 person, 2 toothbrushs\n",
            "Speed: 2.2ms pre-process, 21.0ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 333x500 1 boat\n",
            "Speed: 2.0ms pre-process, 22.4ms inference, 1.3ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 2 persons, 1 book\n",
            "Speed: 2.3ms pre-process, 22.8ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 334x500 1 person, 1 snowboard\n",
            "Speed: 2.2ms pre-process, 23.0ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x333 (no detections)\n",
            "Speed: 3.2ms pre-process, 23.4ms inference, 1.0ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 333x500 (no detections)\n",
            "Speed: 3.0ms pre-process, 22.4ms inference, 0.9ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 dog\n",
            "Speed: 2.2ms pre-process, 22.7ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x333 1 person, 1 skis\n",
            "Speed: 1.8ms pre-process, 22.3ms inference, 1.2ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 332x500 10 persons, 3 chairs, 4 tvs\n",
            "Speed: 2.2ms pre-process, 22.9ms inference, 1.3ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 315x500 1 dog\n",
            "Speed: 1.8ms pre-process, 23.2ms inference, 1.2ms NMS per image at shape (1, 3, 416, 640)\n",
            "image 1/1: 375x500 1 person, 2 cars, 2 traffic lights\n",
            "Speed: 2.4ms pre-process, 24.4ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 281x500 (no detections)\n",
            "Speed: 2.0ms pre-process, 17.7ms inference, 0.9ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 500x335 1 person, 2 buss\n",
            "Speed: 2.9ms pre-process, 22.6ms inference, 1.6ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 383x500 7 persons, 2 bottles\n",
            "Speed: 3.4ms pre-process, 23.4ms inference, 1.6ms NMS per image at shape (1, 3, 512, 640)\n",
            "image 1/1: 500x491 2 persons\n",
            "Speed: 2.7ms pre-process, 29.5ms inference, 1.2ms NMS per image at shape (1, 3, 640, 640)\n",
            "image 1/1: 500x500 1 dog\n",
            "Speed: 2.6ms pre-process, 30.4ms inference, 1.3ms NMS per image at shape (1, 3, 640, 640)\n",
            "image 1/1: 333x500 2 persons\n",
            "Speed: 2.1ms pre-process, 23.8ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 332x500 4 persons, 1 car\n",
            "Speed: 1.8ms pre-process, 23.7ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 4 persons\n",
            "Speed: 2.0ms pre-process, 22.2ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 person, 1 bird, 1 laptop\n",
            "Speed: 2.2ms pre-process, 23.1ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 320x480 2 persons\n",
            "Speed: 2.9ms pre-process, 22.6ms inference, 1.5ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x334 1 person, 1 frisbee\n",
            "Speed: 3.1ms pre-process, 21.4ms inference, 1.5ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 375x500 1 person, 1 kite, 1 tv\n",
            "Speed: 2.4ms pre-process, 22.6ms inference, 1.3ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 3 persons, 1 frisbee, 1 sports ball, 1 book\n",
            "Speed: 2.3ms pre-process, 21.8ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 333x500 2 persons\n",
            "Speed: 2.1ms pre-process, 23.5ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x380 1 person, 1 tennis racket, 1 chair\n",
            "Speed: 3.0ms pre-process, 22.1ms inference, 1.2ms NMS per image at shape (1, 3, 640, 512)\n",
            "image 1/1: 360x500 1 person, 2 birds, 1 dog\n",
            "Speed: 2.1ms pre-process, 22.5ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 333x500 1 person, 1 boat, 1 tennis racket\n",
            "Speed: 1.9ms pre-process, 23.1ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 2 birds, 2 dogs\n",
            "Speed: 2.9ms pre-process, 23.8ms inference, 1.6ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 251x500 16 persons\n",
            "Speed: 2.2ms pre-process, 20.1ms inference, 1.6ms NMS per image at shape (1, 3, 352, 640)\n",
            "image 1/1: 333x500 4 persons, 1 snowboard\n",
            "Speed: 2.2ms pre-process, 23.9ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 person, 2 cars, 1 dog\n",
            "Speed: 2.2ms pre-process, 25.0ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 person, 1 cup, 1 potted plant, 3 tvs, 1 laptop, 2 mouses, 1 keyboard, 1 book\n",
            "Speed: 2.8ms pre-process, 25.6ms inference, 1.3ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 410x500 2 persons\n",
            "Speed: 2.4ms pre-process, 27.3ms inference, 1.3ms NMS per image at shape (1, 3, 544, 640)\n",
            "image 1/1: 332x500 2 persons, 3 chairs\n",
            "Speed: 2.0ms pre-process, 21.9ms inference, 1.3ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x366 1 person\n",
            "Speed: 2.2ms pre-process, 24.6ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 332x500 2 persons, 3 chairs\n",
            "Speed: 2.5ms pre-process, 21.5ms inference, 3.4ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 8 persons\n",
            "Speed: 2.8ms pre-process, 23.6ms inference, 1.7ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 334x500 6 persons, 1 chair\n",
            "Speed: 2.1ms pre-process, 24.2ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 10 persons\n",
            "Speed: 2.0ms pre-process, 24.2ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 385x500 2 persons\n",
            "Speed: 2.2ms pre-process, 25.4ms inference, 1.6ms NMS per image at shape (1, 3, 512, 640)\n",
            "image 1/1: 332x500 4 persons, 1 tv\n",
            "Speed: 2.1ms pre-process, 23.5ms inference, 1.3ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x375 1 person, 1 train, 1 tv\n",
            "Speed: 2.2ms pre-process, 23.3ms inference, 1.2ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 333x500 3 persons, 1 horse, 2 snowboards\n",
            "Speed: 2.9ms pre-process, 36.0ms inference, 1.7ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 440x500 1 person\n",
            "Speed: 3.6ms pre-process, 26.7ms inference, 1.5ms NMS per image at shape (1, 3, 576, 640)\n",
            "image 1/1: 386x500 1 bird\n",
            "Speed: 2.4ms pre-process, 25.3ms inference, 1.2ms NMS per image at shape (1, 3, 512, 640)\n",
            "image 1/1: 333x500 3 persons\n",
            "Speed: 2.0ms pre-process, 23.6ms inference, 1.0ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 313x500 1 person\n",
            "Speed: 2.0ms pre-process, 23.1ms inference, 1.1ms NMS per image at shape (1, 3, 416, 640)\n",
            "image 1/1: 315x500 (no detections)\n",
            "Speed: 1.7ms pre-process, 24.7ms inference, 0.7ms NMS per image at shape (1, 3, 416, 640)\n",
            "image 1/1: 491x500 1 dog\n",
            "Speed: 3.0ms pre-process, 29.9ms inference, 5.7ms NMS per image at shape (1, 3, 640, 640)\n",
            "image 1/1: 500x334 1 person\n",
            "Speed: 2.0ms pre-process, 20.6ms inference, 1.2ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 333x500 11 persons, 1 bicycle\n",
            "Speed: 2.8ms pre-process, 23.2ms inference, 1.5ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x342 1 person\n",
            "Speed: 2.9ms pre-process, 22.6ms inference, 1.5ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x334 (no detections)\n",
            "Speed: 2.0ms pre-process, 21.8ms inference, 1.4ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x364 1 person, 1 train, 1 frisbee, 1 snowboard\n",
            "Speed: 2.2ms pre-process, 21.7ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 375x500 7 persons, 1 oven\n",
            "Speed: 1.9ms pre-process, 22.2ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x375 6 persons, 1 tv\n",
            "Speed: 2.0ms pre-process, 22.1ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 375x500 1 person\n",
            "Speed: 2.1ms pre-process, 23.8ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 toilet\n",
            "Speed: 2.2ms pre-process, 22.8ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 332x500 2 persons, 1 knife, 1 cake, 1 dining table\n",
            "Speed: 3.1ms pre-process, 25.5ms inference, 1.5ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 372x500 1 person, 3 scissorss\n",
            "Speed: 2.8ms pre-process, 23.9ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 person, 1 cat\n",
            "Speed: 2.0ms pre-process, 22.7ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 333x500 1 bird, 1 sports ball, 1 kite\n",
            "Speed: 2.0ms pre-process, 22.3ms inference, 1.4ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 328x500 4 persons, 1 truck\n",
            "Speed: 1.9ms pre-process, 22.4ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x345 1 person\n",
            "Speed: 2.0ms pre-process, 21.0ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 340x500 1 truck\n",
            "Speed: 1.9ms pre-process, 23.9ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x332 1 person\n",
            "Speed: 1.9ms pre-process, 21.3ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 337x500 1 person, 1 frisbee\n",
            "Speed: 2.6ms pre-process, 22.4ms inference, 1.4ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x402 1 motorcycle\n",
            "Speed: 3.3ms pre-process, 26.1ms inference, 1.5ms NMS per image at shape (1, 3, 640, 544)\n",
            "image 1/1: 400x500 2 persons\n",
            "Speed: 2.4ms pre-process, 23.0ms inference, 1.1ms NMS per image at shape (1, 3, 512, 640)\n",
            "image 1/1: 334x500 (no detections)\n",
            "Speed: 1.8ms pre-process, 23.3ms inference, 0.7ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x333 2 persons, 1 cup, 7 chairs\n",
            "Speed: 2.2ms pre-process, 21.6ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 281x500 2 persons\n",
            "Speed: 1.7ms pre-process, 18.7ms inference, 1.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 333x500 3 persons\n",
            "Speed: 1.9ms pre-process, 22.4ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 1 person\n",
            "Speed: 1.9ms pre-process, 23.5ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 370x500 1 person\n",
            "Speed: 3.0ms pre-process, 23.1ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x333 6 persons, 2 umbrellas\n",
            "Speed: 2.7ms pre-process, 24.6ms inference, 1.5ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 330x500 2 persons\n",
            "Speed: 2.2ms pre-process, 24.2ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 299x500 3 persons\n",
            "Speed: 2.0ms pre-process, 19.6ms inference, 1.2ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 333x500 5 persons, 1 train, 1 chair\n",
            "Speed: 2.2ms pre-process, 22.4ms inference, 1.3ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x281 1 person\n",
            "Speed: 1.7ms pre-process, 19.4ms inference, 1.0ms NMS per image at shape (1, 3, 640, 384)\n",
            "image 1/1: 332x500 3 persons\n",
            "Speed: 1.9ms pre-process, 22.0ms inference, 1.7ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 374x500 2 persons, 1 boat\n",
            "Speed: 2.0ms pre-process, 22.9ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x462 1 person, 1 frisbee, 1 sports ball\n",
            "Speed: 3.8ms pre-process, 28.2ms inference, 1.4ms NMS per image at shape (1, 3, 640, 608)\n",
            "image 1/1: 334x500 (no detections)\n",
            "Speed: 2.6ms pre-process, 22.7ms inference, 0.9ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x332 2 persons, 3 donuts\n",
            "Speed: 2.2ms pre-process, 21.4ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x375 1 person, 1 tv\n",
            "Speed: 2.0ms pre-process, 23.3ms inference, 1.2ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 375x500 2 persons, 1 dog, 1 skateboard\n",
            "Speed: 2.2ms pre-process, 23.2ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 333x500 4 persons\n",
            "Speed: 1.9ms pre-process, 24.2ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 3 persons, 1 dog\n",
            "Speed: 2.1ms pre-process, 23.1ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 2 persons, 1 bench, 1 dog, 1 skis, 1 surfboard, 1 book\n",
            "Speed: 2.0ms pre-process, 23.4ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 333x500 2 dogs\n",
            "Speed: 4.5ms pre-process, 22.7ms inference, 1.4ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 339x500 5 persons, 1 handbag\n",
            "Speed: 2.8ms pre-process, 21.7ms inference, 1.4ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 person\n",
            "Speed: 2.3ms pre-process, 23.7ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 333x500 1 dog, 1 bear, 1 surfboard\n",
            "Speed: 2.4ms pre-process, 22.2ms inference, 1.0ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x334 1 person\n",
            "Speed: 2.1ms pre-process, 22.4ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 349x500 1 dog\n",
            "Speed: 2.0ms pre-process, 23.3ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x332 1 person\n",
            "Speed: 2.1ms pre-process, 22.0ms inference, 1.2ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 375x500 2 persons, 1 laptop\n",
            "Speed: 2.0ms pre-process, 23.8ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x375 1 person, 1 kite\n",
            "Speed: 2.9ms pre-process, 22.0ms inference, 1.7ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 375x500 2 persons, 1 tie, 1 potted plant\n",
            "Speed: 2.7ms pre-process, 22.5ms inference, 1.7ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 person\n",
            "Speed: 2.4ms pre-process, 24.5ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 4 persons, 1 tie, 1 bottle, 2 chairs, 1 dining table\n",
            "Speed: 2.2ms pre-process, 22.7ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 332x500 1 person\n",
            "Speed: 1.9ms pre-process, 22.9ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x375 4 persons\n",
            "Speed: 2.2ms pre-process, 21.3ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 375x500 4 persons, 2 frisbees\n",
            "Speed: 2.1ms pre-process, 23.6ms inference, 1.3ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 344x500 1 person, 1 tv\n",
            "Speed: 1.9ms pre-process, 23.1ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 1 person, 1 bicycle\n",
            "Speed: 2.6ms pre-process, 25.0ms inference, 1.6ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x333 1 person\n",
            "Speed: 2.7ms pre-process, 21.8ms inference, 1.7ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 355x500 22 persons, 2 frisbees\n",
            "Speed: 2.1ms pre-process, 22.3ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x333 1 person, 1 boat, 1 traffic light, 1 tv\n",
            "Speed: 2.0ms pre-process, 23.2ms inference, 1.5ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x333 1 person, 1 surfboard\n",
            "Speed: 2.2ms pre-process, 22.4ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x375 (no detections)\n",
            "Speed: 2.0ms pre-process, 21.4ms inference, 0.7ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 333x500 1 person\n",
            "Speed: 2.1ms pre-process, 22.7ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 person, 2 birds, 2 dogs\n",
            "Speed: 2.0ms pre-process, 23.8ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 357x500 1 dog\n",
            "Speed: 3.1ms pre-process, 23.2ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 person, 1 car, 1 motorcycle\n",
            "Speed: 2.8ms pre-process, 21.9ms inference, 1.6ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 12 persons, 1 chair\n",
            "Speed: 2.0ms pre-process, 23.5ms inference, 1.3ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 350x500 1 person\n",
            "Speed: 1.9ms pre-process, 22.4ms inference, 1.0ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 334x500 2 persons, 1 banana\n",
            "Speed: 2.0ms pre-process, 22.6ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 5 persons\n",
            "Speed: 2.5ms pre-process, 21.6ms inference, 1.0ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 331x500 8 persons\n",
            "Speed: 2.0ms pre-process, 23.0ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 332x500 2 persons, 2 cups, 1 tv, 1 refrigerator\n",
            "Speed: 2.0ms pre-process, 22.8ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x375 (no detections)\n",
            "Speed: 2.9ms pre-process, 26.0ms inference, 1.4ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 375x500 2 persons, 1 handbag\n",
            "Speed: 2.8ms pre-process, 24.0ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 person, 1 dog\n",
            "Speed: 2.1ms pre-process, 22.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 333x500 1 person\n",
            "Speed: 1.8ms pre-process, 23.2ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 334x500 4 persons, 1 boat, 1 backpack\n",
            "Speed: 2.3ms pre-process, 23.5ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 300x500 3 persons, 1 bottle\n",
            "Speed: 1.8ms pre-process, 19.3ms inference, 1.5ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 500x375 1 person, 1 truck, 1 knife, 3 spoons, 3 bowls\n",
            "Speed: 2.2ms pre-process, 21.1ms inference, 1.2ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 333x500 1 person, 3 birds, 1 wine glass\n",
            "Speed: 2.0ms pre-process, 23.1ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 318x500 4 persons, 2 boats, 1 frisbee\n",
            "Speed: 3.3ms pre-process, 23.7ms inference, 1.4ms NMS per image at shape (1, 3, 416, 640)\n",
            "image 1/1: 500x333 (no detections)\n",
            "Speed: 2.5ms pre-process, 21.4ms inference, 1.0ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 375x500 8 persons\n",
            "Speed: 2.1ms pre-process, 23.3ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x334 2 persons\n",
            "Speed: 2.2ms pre-process, 21.7ms inference, 2.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x335 2 persons\n",
            "Speed: 2.1ms pre-process, 21.6ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x500 4 persons, 1 baseball bat\n",
            "Speed: 2.6ms pre-process, 29.5ms inference, 1.1ms NMS per image at shape (1, 3, 640, 640)\n",
            "image 1/1: 333x500 5 persons, 1 baseball glove\n",
            "Speed: 1.9ms pre-process, 23.9ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 person, 5 cars\n",
            "Speed: 3.1ms pre-process, 23.3ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 2 dogs, 1 chair\n",
            "Speed: 2.8ms pre-process, 23.9ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 331x500 3 persons\n",
            "Speed: 1.9ms pre-process, 22.4ms inference, 1.0ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x375 1 person\n",
            "Speed: 2.1ms pre-process, 22.4ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 500x352 1 person\n",
            "Speed: 2.3ms pre-process, 23.1ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 333x500 1 person, 1 bench, 1 surfboard\n",
            "Speed: 1.8ms pre-process, 21.8ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x375 1 person\n",
            "Speed: 2.1ms pre-process, 23.3ms inference, 1.2ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 332x500 1 dog, 1 elephant\n",
            "Speed: 2.2ms pre-process, 22.0ms inference, 1.4ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x333 2 persons, 1 surfboard\n",
            "Speed: 2.7ms pre-process, 22.7ms inference, 1.5ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x375 2 persons\n",
            "Speed: 2.8ms pre-process, 22.3ms inference, 1.4ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 333x500 7 persons\n",
            "Speed: 2.0ms pre-process, 23.1ms inference, 1.3ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 383x500 (no detections)\n",
            "Speed: 2.1ms pre-process, 25.0ms inference, 0.4ms NMS per image at shape (1, 3, 512, 640)\n",
            "image 1/1: 500x342 1 person\n",
            "Speed: 2.2ms pre-process, 21.7ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x333 (no detections)\n",
            "Speed: 1.8ms pre-process, 20.6ms inference, 0.7ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 313x500 1 cat, 2 dogs\n",
            "Speed: 1.8ms pre-process, 24.7ms inference, 1.1ms NMS per image at shape (1, 3, 416, 640)\n",
            "image 1/1: 333x500 1 person, 1 refrigerator\n",
            "Speed: 2.1ms pre-process, 22.3ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 365x500 1 person, 1 bicycle\n",
            "Speed: 2.9ms pre-process, 23.8ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 333x500 1 person, 1 bottle, 1 tv\n",
            "Speed: 2.5ms pre-process, 22.5ms inference, 1.4ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 449x500 2 persons, 1 truck, 1 refrigerator\n",
            "Speed: 2.5ms pre-process, 27.4ms inference, 1.1ms NMS per image at shape (1, 3, 576, 640)\n",
            "image 1/1: 333x500 1 car, 1 dog\n",
            "Speed: 1.9ms pre-process, 22.7ms inference, 1.3ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 245x300 2 persons, 1 tennis racket\n",
            "Speed: 2.4ms pre-process, 27.5ms inference, 1.1ms NMS per image at shape (1, 3, 544, 640)\n",
            "image 1/1: 307x500 4 persons, 1 chair\n",
            "Speed: 2.1ms pre-process, 25.0ms inference, 1.6ms NMS per image at shape (1, 3, 416, 640)\n",
            "image 1/1: 375x500 1 person, 1 sports ball, 1 tennis racket\n",
            "Speed: 2.3ms pre-process, 22.2ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 333x500 3 persons, 1 motorcycle, 1 frisbee\n",
            "Speed: 2.1ms pre-process, 23.2ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 1 person, 1 dog\n",
            "Speed: 2.6ms pre-process, 21.7ms inference, 1.5ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x375 1 person, 1 skateboard\n",
            "Speed: 2.8ms pre-process, 20.9ms inference, 1.4ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 500x375 1 person\n",
            "Speed: 2.2ms pre-process, 23.3ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 500x334 1 person\n",
            "Speed: 2.5ms pre-process, 21.7ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 333x500 4 persons, 1 wine glass, 4 cups, 1 spoon, 1 bowl, 1 dining table\n",
            "Speed: 2.0ms pre-process, 22.5ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x358 7 persons\n",
            "Speed: 2.1ms pre-process, 23.0ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 333x500 2 persons, 2 tvs\n",
            "Speed: 2.2ms pre-process, 23.5ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 person\n",
            "Speed: 2.1ms pre-process, 23.5ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x351 1 person, 1 surfboard\n",
            "Speed: 2.8ms pre-process, 22.1ms inference, 1.8ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 400x500 1 person, 1 stop sign, 2 bottles\n",
            "Speed: 2.9ms pre-process, 23.5ms inference, 1.4ms NMS per image at shape (1, 3, 512, 640)\n",
            "image 1/1: 406x500 3 persons, 1 bench\n",
            "Speed: 2.3ms pre-process, 27.9ms inference, 1.2ms NMS per image at shape (1, 3, 544, 640)\n",
            "image 1/1: 306x500 5 persons, 1 bowl, 2 chairs\n",
            "Speed: 2.2ms pre-process, 24.9ms inference, 1.2ms NMS per image at shape (1, 3, 416, 640)\n",
            "image 1/1: 333x500 2 persons, 1 bicycle\n",
            "Speed: 2.1ms pre-process, 22.4ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 2 persons\n",
            "Speed: 2.5ms pre-process, 23.2ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 252x500 1 dog, 2 frisbees\n",
            "Speed: 1.5ms pre-process, 19.5ms inference, 1.1ms NMS per image at shape (1, 3, 352, 640)\n",
            "image 1/1: 333x500 2 persons, 1 chair\n",
            "Speed: 1.9ms pre-process, 23.2ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 person, 1 bottle, 1 chair, 1 sink\n",
            "Speed: 2.7ms pre-process, 22.3ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 317x500 1 person, 1 snowboard, 1 surfboard\n",
            "Speed: 2.5ms pre-process, 22.8ms inference, 1.5ms NMS per image at shape (1, 3, 416, 640)\n",
            "image 1/1: 332x500 1 person, 7 cars, 1 truck\n",
            "Speed: 2.1ms pre-process, 22.2ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 337x500 1 person, 1 frisbee\n",
            "Speed: 1.9ms pre-process, 22.9ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 453x500 2 persons, 3 cakes, 1 dining table\n",
            "Speed: 2.6ms pre-process, 28.0ms inference, 1.1ms NMS per image at shape (1, 3, 608, 640)\n",
            "image 1/1: 308x500 1 person, 1 dog\n",
            "Speed: 2.3ms pre-process, 24.0ms inference, 1.2ms NMS per image at shape (1, 3, 416, 640)\n",
            "image 1/1: 500x426 2 dogs\n",
            "Speed: 2.4ms pre-process, 28.1ms inference, 1.6ms NMS per image at shape (1, 3, 640, 576)\n",
            "image 1/1: 356x500 2 persons, 1 cake, 1 dining table\n",
            "Speed: 2.2ms pre-process, 22.6ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x396 4 persons, 2 trains, 1 cake\n",
            "Speed: 3.3ms pre-process, 23.6ms inference, 1.5ms NMS per image at shape (1, 3, 640, 512)\n",
            "image 1/1: 500x330 1 dog, 1 kite\n",
            "Speed: 2.6ms pre-process, 23.4ms inference, 1.3ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 375x500 1 person, 1 bed\n",
            "Speed: 2.1ms pre-process, 22.4ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 299x500 4 persons, 2 trucks\n",
            "Speed: 2.7ms pre-process, 19.8ms inference, 1.3ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 500x332 1 person\n",
            "Speed: 2.4ms pre-process, 22.2ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 464x500 2 persons\n",
            "Speed: 2.7ms pre-process, 28.4ms inference, 1.1ms NMS per image at shape (1, 3, 608, 640)\n",
            "image 1/1: 375x500 1 person, 4 surfboards, 1 cup, 1 bowl\n",
            "Speed: 2.1ms pre-process, 23.7ms inference, 1.6ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 281x500 1 person, 1 bowl, 1 dining table\n",
            "Speed: 2.6ms pre-process, 22.5ms inference, 1.6ms NMS per image at shape (1, 3, 384, 640)\n",
            "image 1/1: 500x333 3 persons\n",
            "Speed: 2.6ms pre-process, 20.4ms inference, 1.4ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 412x500 2 persons, 1 laptop\n",
            "Speed: 2.4ms pre-process, 27.2ms inference, 1.1ms NMS per image at shape (1, 3, 544, 640)\n",
            "image 1/1: 375x500 2 persons, 2 umbrellas, 1 handbag, 1 potted plant\n",
            "Speed: 2.0ms pre-process, 22.6ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 2 persons, 1 horse, 1 chair\n",
            "Speed: 1.9ms pre-process, 25.5ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 elephant\n",
            "Speed: 2.0ms pre-process, 24.4ms inference, 2.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 9 persons\n",
            "Speed: 2.2ms pre-process, 24.1ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 335x500 2 dogs\n",
            "Speed: 2.0ms pre-process, 24.1ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 1 dog\n",
            "Speed: 2.7ms pre-process, 22.5ms inference, 1.4ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x335 2 persons\n",
            "Speed: 2.9ms pre-process, 21.2ms inference, 1.4ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 333x500 1 person, 1 truck\n",
            "Speed: 2.1ms pre-process, 21.9ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 333x500 1 person, 1 refrigerator\n",
            "Speed: 1.9ms pre-process, 22.6ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 467x500 1 train\n",
            "Speed: 2.5ms pre-process, 28.9ms inference, 1.1ms NMS per image at shape (1, 3, 608, 640)\n",
            "image 1/1: 375x500 7 persons\n",
            "Speed: 2.1ms pre-process, 24.4ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 person\n",
            "Speed: 2.0ms pre-process, 23.3ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 person, 1 wine glass, 2 bowls, 1 tv, 1 oven\n",
            "Speed: 2.1ms pre-process, 22.9ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 356x500 6 persons, 2 birds, 1 tennis racket\n",
            "Speed: 2.8ms pre-process, 21.5ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x333 1 bed\n",
            "Speed: 2.6ms pre-process, 23.7ms inference, 1.5ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x375 1 person, 1 cake\n",
            "Speed: 1.9ms pre-process, 22.7ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 375x500 11 persons, 1 horse, 1 cake\n",
            "Speed: 2.1ms pre-process, 24.7ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x333 6 persons, 1 bottle\n",
            "Speed: 2.0ms pre-process, 22.5ms inference, 2.9ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x333 1 person\n",
            "Speed: 1.9ms pre-process, 23.3ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 332x500 1 person, 1 bed\n",
            "Speed: 2.0ms pre-process, 22.5ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x333 1 person, 1 bottle, 1 bowl\n",
            "Speed: 2.1ms pre-process, 22.9ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 375x500 2 persons, 1 skis\n",
            "Speed: 2.7ms pre-process, 23.5ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x500 1 person, 1 bench, 1 chair\n",
            "Speed: 3.7ms pre-process, 28.4ms inference, 2.9ms NMS per image at shape (1, 3, 640, 640)\n",
            "image 1/1: 500x375 1 dog\n",
            "Speed: 1.9ms pre-process, 21.6ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 500x334 1 person\n",
            "Speed: 2.3ms pre-process, 22.7ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x400 1 person, 1 bed, 1 laptop\n",
            "Speed: 2.2ms pre-process, 22.4ms inference, 1.1ms NMS per image at shape (1, 3, 640, 512)\n",
            "image 1/1: 453x500 3 persons, 1 truck, 1 bench\n",
            "Speed: 2.5ms pre-process, 28.8ms inference, 1.3ms NMS per image at shape (1, 3, 608, 640)\n",
            "image 1/1: 412x500 1 person, 1 toothbrush\n",
            "Speed: 2.4ms pre-process, 26.2ms inference, 1.1ms NMS per image at shape (1, 3, 544, 640)\n",
            "image 1/1: 500x375 1 person\n",
            "Speed: 2.0ms pre-process, 24.2ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 348x500 1 person\n",
            "Speed: 3.6ms pre-process, 22.7ms inference, 1.5ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 person\n",
            "Speed: 2.8ms pre-process, 22.5ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 person, 2 cups, 1 dining table, 1 cell phone\n",
            "Speed: 2.0ms pre-process, 24.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 336x500 1 truck, 1 tv\n",
            "Speed: 1.8ms pre-process, 24.1ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 2 persons, 1 tv\n",
            "Speed: 2.0ms pre-process, 24.1ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 (no detections)\n",
            "Speed: 2.4ms pre-process, 22.8ms inference, 0.3ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x375 1 person, 1 cow, 1 elephant\n",
            "Speed: 1.9ms pre-process, 21.5ms inference, 1.0ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 333x500 2 persons, 1 umbrella\n",
            "Speed: 1.9ms pre-process, 22.9ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x349 3 persons, 1 bus, 1 train\n",
            "Speed: 2.7ms pre-process, 21.5ms inference, 1.4ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x333 1 person, 1 car\n",
            "Speed: 2.7ms pre-process, 23.6ms inference, 1.4ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 375x500 2 persons, 1 bird, 1 dog, 1 kite, 2 cakes\n",
            "Speed: 2.2ms pre-process, 25.0ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 2 birds\n",
            "Speed: 2.2ms pre-process, 24.0ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x380 2 persons, 1 handbag, 1 tv, 1 refrigerator\n",
            "Speed: 2.5ms pre-process, 22.0ms inference, 1.2ms NMS per image at shape (1, 3, 640, 512)\n",
            "image 1/1: 333x500 3 persons\n",
            "Speed: 2.2ms pre-process, 22.7ms inference, 1.1ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 500x375 1 toilet, 2 teddy bears\n",
            "Speed: 2.1ms pre-process, 22.2ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 400x500 1 person, 1 bowl, 1 chair, 1 potted plant\n",
            "Speed: 2.2ms pre-process, 23.1ms inference, 1.1ms NMS per image at shape (1, 3, 512, 640)\n",
            "image 1/1: 500x333 1 person\n",
            "Speed: 2.7ms pre-process, 22.3ms inference, 1.4ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 375x500 1 person\n",
            "Speed: 2.8ms pre-process, 22.8ms inference, 1.9ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 332x500 1 person, 1 tennis racket, 1 chair\n",
            "Speed: 2.4ms pre-process, 24.1ms inference, 1.3ms NMS per image at shape (1, 3, 448, 640)\n",
            "image 1/1: 375x500 1 person\n",
            "Speed: 2.1ms pre-process, 24.5ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x333 1 bird, 1 surfboard\n",
            "Speed: 2.3ms pre-process, 22.0ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 375x500 4 persons, 2 bicycles, 1 car\n",
            "Speed: 2.1ms pre-process, 22.8ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 2 persons\n",
            "Speed: 1.9ms pre-process, 24.0ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 375x500 1 tennis racket\n",
            "Speed: 2.0ms pre-process, 22.7ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
            "image 1/1: 500x305 3 persons\n",
            "Speed: 2.5ms pre-process, 20.7ms inference, 1.4ms NMS per image at shape (1, 3, 640, 416)\n",
            "image 1/1: 500x333 1 person, 1 snowboard, 1 vase\n",
            "Speed: 2.5ms pre-process, 23.4ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 440x500 2 persons, 1 car\n",
            "Speed: 2.8ms pre-process, 28.2ms inference, 1.2ms NMS per image at shape (1, 3, 576, 640)\n",
            "image 1/1: 500x333 1 person\n",
            "Speed: 2.1ms pre-process, 20.7ms inference, 1.1ms NMS per image at shape (1, 3, 640, 448)\n",
            "image 1/1: 500x375 2 persons\n",
            "Speed: 2.0ms pre-process, 22.6ms inference, 1.1ms NMS per image at shape (1, 3, 640, 480)\n",
            "image 1/1: 333x500 3 persons, 2 bicycles, 2 cars, 1 truck, 1 stop sign\n",
            "Speed: 2.0ms pre-process, 23.8ms inference, 1.2ms NMS per image at shape (1, 3, 448, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "for key in img_to_desc.keys():\n",
        "  if idx == 10:\n",
        "    break\n",
        "  print(img_to_desc[key])\n",
        "  idx+=1\n",
        "print(len(img_to_desc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JxuiK1cZ4ID",
        "outputId": "72e841ef-e9a0-4e9a-9889-8131f37dcf31"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 8.84769e+00, -3.99620e+00,  2.95633e-02,  ...,  2.11479e-23,  7.88639e-24,  4.61102e-26],\n",
            "         [ 1.90158e+01, -3.99986e+00,  9.89480e+00,  ...,  2.87215e-21,  1.48644e-23,  6.86534e-24],\n",
            "         [ 2.65121e+01, -3.99996e+00,  9.25027e+00,  ...,  1.23030e-19,  3.89859e-21,  3.57763e-21],\n",
            "         ...,\n",
            "         [ 5.59501e+02,  6.01998e+02,  1.70304e+02,  ...,  5.29391e-05,  1.86155e-04,  2.20982e-04],\n",
            "         [ 5.84914e+02,  6.00486e+02,  1.45972e+02,  ...,  4.53064e-05,  1.90183e-04,  2.31097e-04],\n",
            "         [ 6.25489e+02,  6.12057e+02,  1.33453e+02,  ...,  5.04580e-05,  1.71968e-04,  1.74073e-04]]], device='cuda:0')\n",
            "tensor([[[ 1.08440e+01, -3.92130e+00,  1.61481e+01,  ...,  4.92917e-18,  9.39190e-17,  1.45959e-19],\n",
            "         [ 1.14706e+01,  5.56011e+00,  7.82973e-03,  ...,  2.62637e-16,  4.67578e-18,  2.97430e-18],\n",
            "         [ 2.72359e+01, -3.99925e+00,  1.44523e+01,  ...,  6.01364e-20,  7.81233e-20,  4.91281e-22],\n",
            "         ...,\n",
            "         [ 5.57120e+02,  6.01755e+02,  1.81435e+02,  ...,  2.23278e-04,  3.28533e-04,  3.45718e-04],\n",
            "         [ 5.85688e+02,  6.05158e+02,  1.42358e+02,  ...,  6.78498e-04,  4.76168e-04,  5.13415e-04],\n",
            "         [ 6.16658e+02,  6.12089e+02,  1.41217e+02,  ...,  2.75733e-04,  4.04797e-04,  4.37787e-04]]], device='cuda:0')\n",
            "tensor([[[7.91935e+00, 4.19446e+00, 1.94552e+01,  ..., 2.13808e-09, 3.18837e-09, 2.62275e-09],\n",
            "         [1.52123e+01, 5.17726e+00, 1.83152e+01,  ..., 6.94263e-09, 2.70900e-09, 1.63634e-09],\n",
            "         [1.65359e+01, 4.24389e+00, 2.80103e+01,  ..., 2.02822e-07, 1.62320e-08, 1.01504e-07],\n",
            "         ...,\n",
            "         [5.62323e+02, 5.97616e+02, 1.80721e+02,  ..., 4.97785e-05, 1.58630e-04, 1.67170e-04],\n",
            "         [5.88602e+02, 5.99600e+02, 1.59069e+02,  ..., 4.99091e-05, 1.75252e-04, 1.85852e-04],\n",
            "         [6.21961e+02, 6.15062e+02, 1.38841e+02,  ..., 1.91289e-04, 3.39138e-04, 3.79294e-04]]], device='cuda:0')\n",
            "tensor([[[9.03577e+00, 8.50391e-02, 1.81554e+01,  ..., 3.71057e-06, 1.35886e-06, 7.86981e-07],\n",
            "         [1.39397e+01, 1.59309e+00, 1.69581e+01,  ..., 9.26275e-06, 2.63732e-06, 1.51002e-06],\n",
            "         [2.08923e+01, 3.94666e+00, 2.42281e+01,  ..., 4.18195e-06, 1.55767e-06, 8.53458e-07],\n",
            "         ...,\n",
            "         [5.61294e+02, 6.06602e+02, 1.90162e+02,  ..., 1.39699e-04, 2.91161e-04, 3.07210e-04],\n",
            "         [5.88613e+02, 6.06837e+02, 1.69490e+02,  ..., 1.62921e-04, 2.97437e-04, 2.90346e-04],\n",
            "         [6.22996e+02, 6.14633e+02, 1.40809e+02,  ..., 2.31496e-04, 4.41143e-04, 4.40015e-04]]], device='cuda:0')\n",
            "tensor([[[ 7.29572e+00, -9.13527e-02,  9.60968e+00,  ...,  1.20056e-08,  5.87094e-09,  4.64563e-08],\n",
            "         [ 1.61487e+01, -2.86805e+00,  2.33099e+01,  ...,  7.57049e-10,  4.80784e-10,  3.85515e-09],\n",
            "         [ 1.95534e+01, -3.47174e+00,  2.67488e+01,  ...,  1.09047e-11,  7.96321e-12,  9.33774e-11],\n",
            "         ...,\n",
            "         [ 5.58570e+02,  6.01384e+02,  1.82043e+02,  ...,  1.33857e-05,  8.51599e-05,  7.55376e-05],\n",
            "         [ 5.90066e+02,  6.00499e+02,  1.90272e+02,  ...,  1.12300e-05,  7.45710e-05,  6.26326e-05],\n",
            "         [ 6.25728e+02,  6.12772e+02,  1.99049e+02,  ...,  1.21740e-05,  7.38545e-05,  6.08734e-05]]], device='cuda:0')\n",
            "tensor([[[ 1.15364e+01, -3.97374e+00,  4.87016e+00,  ...,  4.81891e-24,  5.19552e-23,  3.85463e-26],\n",
            "         [ 1.93536e+01, -3.99826e+00,  2.44203e+01,  ...,  5.61183e-23,  2.30359e-24,  1.29993e-24],\n",
            "         [ 2.54677e+01, -3.99900e+00,  8.25992e+00,  ...,  9.33583e-24,  7.89440e-25,  6.40951e-25],\n",
            "         ...,\n",
            "         [ 5.61793e+02,  5.97715e+02,  1.63324e+02,  ...,  1.58994e-06,  2.76839e-05,  2.74980e-05],\n",
            "         [ 5.94322e+02,  5.97945e+02,  1.27515e+02,  ...,  1.98005e-06,  3.40765e-05,  3.26214e-05],\n",
            "         [ 6.26291e+02,  6.11184e+02,  1.37257e+02,  ...,  1.28463e-05,  8.38378e-05,  8.66841e-05]]], device='cuda:0')\n",
            "tensor([[[ 1.02027e+01,  1.31167e+00,  7.77547e+00,  ...,  5.42180e-11,  1.60338e-11,  2.89965e-11],\n",
            "         [ 1.80295e+01, -2.99802e+00,  3.17408e+01,  ...,  2.66053e-25,  5.70929e-25,  4.47985e-25],\n",
            "         [ 2.21394e+01,  7.62240e+00,  3.14626e+01,  ...,  1.05456e-13,  5.65793e-15,  8.54578e-14],\n",
            "         ...,\n",
            "         [ 5.66750e+02,  5.94738e+02,  1.37766e+02,  ...,  3.93541e-06,  5.21982e-05,  5.49482e-05],\n",
            "         [ 5.97167e+02,  5.96546e+02,  1.41794e+02,  ...,  1.81103e-06,  3.93324e-05,  3.94221e-05],\n",
            "         [ 6.28193e+02,  6.12780e+02,  1.44596e+02,  ...,  4.86438e-06,  6.12502e-05,  5.52146e-05]]], device='cuda:0')\n",
            "tensor([[[ 3.38994e+00, -6.70961e-01,  1.49032e+01,  ...,  4.64276e-08,  6.35286e-08,  3.30612e-08],\n",
            "         [ 1.20554e+01, -3.22285e+00,  8.11617e+00,  ...,  4.02237e-07,  2.82997e-07,  2.89365e-07],\n",
            "         [ 2.43123e+01, -3.09891e+00,  1.30409e+01,  ...,  5.99349e-12,  2.15847e-12,  1.03410e-11],\n",
            "         ...,\n",
            "         [ 5.64417e+02,  5.97308e+02,  1.44237e+02,  ...,  2.74786e-07,  1.09276e-05,  9.53553e-06],\n",
            "         [ 5.91110e+02,  5.97415e+02,  1.56421e+02,  ...,  1.69564e-06,  2.69154e-05,  2.54994e-05],\n",
            "         [ 6.24787e+02,  6.12373e+02,  1.47925e+02,  ...,  2.33598e-05,  1.11468e-04,  1.13874e-04]]], device='cuda:0')\n",
            "tensor([[[ 1.14928e+01, -3.96406e+00,  3.98513e+00,  ...,  3.08557e-19,  1.28321e-21,  1.70087e-19],\n",
            "         [ 1.87438e+01, -3.99238e+00,  2.11509e+01,  ...,  3.38720e-17,  5.62643e-19,  7.67539e-18],\n",
            "         [ 2.63823e+01, -3.99914e+00,  4.13194e+00,  ...,  1.82909e-16,  1.53579e-18,  3.41263e-17],\n",
            "         ...,\n",
            "         [ 5.61337e+02,  5.97008e+02,  1.79549e+02,  ...,  8.96505e-06,  6.82035e-05,  6.60164e-05],\n",
            "         [ 5.90492e+02,  5.99269e+02,  1.75819e+02,  ...,  6.81618e-06,  6.40974e-05,  6.09094e-05],\n",
            "         [ 6.26560e+02,  6.11805e+02,  1.44194e+02,  ...,  2.35226e-05,  1.22140e-04,  1.18571e-04]]], device='cuda:0')\n",
            "tensor([[[ 1.12623e+01, -3.90225e+00,  3.46694e+00,  ...,  8.97018e-20,  1.64482e-19,  2.91588e-20],\n",
            "         [ 1.88675e+01, -3.98056e+00,  2.48176e+01,  ...,  4.46481e-25,  1.39661e-24,  4.77328e-25],\n",
            "         [ 2.59190e+01, -3.73576e+00,  2.34316e+01,  ...,  2.67310e-30,  2.39263e-29,  3.06734e-30],\n",
            "         ...,\n",
            "         [ 5.59714e+02,  5.99332e+02,  1.78154e+02,  ...,  2.23352e-04,  3.41070e-04,  3.76030e-04],\n",
            "         [ 5.90328e+02,  5.99135e+02,  1.61634e+02,  ...,  9.08987e-05,  2.13544e-04,  2.14808e-04],\n",
            "         [ 6.21573e+02,  6.06888e+02,  1.46163e+02,  ...,  1.94912e-04,  3.87938e-04,  4.10851e-04]]], device='cuda:0')\n",
            "1321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(ds_path)))\n",
        "print(len(img_to_desc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6--6pL0ivQD",
        "outputId": "a9ba5e0d-ccd1-43d6-bfce-433c34fc323a"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14237\n",
            "14200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obj_to_index = {'person': 0, 'bicycle': 1, 'car': 2, 'motorcycle': 3, 'airplane': 4, 'bus': 5, 'train': 6, 'truck': 7, 'boat': 8, 'traffic-light': 9, 'fire-hydrant': 10, 'stop-sign': 11, 'parking-meter': 12, 'bench': 13, 'bird': 14, 'cat': 15, 'dog': 16, 'horse': 17, 'sheep': 18, 'cow': 19, 'elephant': 20, 'bear': 21, 'zebra': 22, 'giraffe': 23, 'backpack': 24, 'umbrella': 25, 'handbag': 26, 'tie': 27, 'suitcase': 28, 'frisbee': 29, 'skis': 30, 'snowboard': 31, 'sports-ball': 32, 'kite': 33, 'baseball-bat': 34, 'baseball-glove': 35, 'skateboard': 36, 'surfboard': 37, 'tennis-racket': 38, 'bottle': 39, 'wine-glass': 40, 'cup': 41, 'fork': 42, 'knife': 43, 'spoon': 44, 'bowl': 45, 'banana': 46, 'apple': 47, 'sandwich': 48, 'orange': 49, 'broccoli': 50, 'carrot': 51, 'hot-dog': 52, 'pizza': 53, 'donut': 54, 'cake': 55, 'chair': 56, 'couch': 57, 'potted-plant': 58, 'bed': 59, 'dining-table': 60, 'toilet': 61, 'tv': 62, 'laptop': 63, 'mouse': 64, 'remote': 65, 'keyboard': 66, 'cell-phone': 67, 'microwave': 68, 'oven': 69, 'toaster': 70, 'sink': 71, 'refrigerator': 72, 'book': 73, 'clock': 74, 'vase': 75, 'scissors': 76, 'teddy-bear': 77, 'hair-drier': 78, 'toothbrush': 79\n",
        "}\n",
        "print(obj_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwxsczfyu5pY",
        "outputId": "8934098a-5e42-4b48-c29f-09aa315079ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'person': 0, 'bicycle': 1, 'car': 2, 'motorcycle': 3, 'airplane': 4, 'bus': 5, 'train': 6, 'truck': 7, 'boat': 8, 'traffic-light': 9, 'fire-hydrant': 10, 'stop-sign': 11, 'parking-meter': 12, 'bench': 13, 'bird': 14, 'cat': 15, 'dog': 16, 'horse': 17, 'sheep': 18, 'cow': 19, 'elephant': 20, 'bear': 21, 'zebra': 22, 'giraffe': 23, 'backpack': 24, 'umbrella': 25, 'handbag': 26, 'tie': 27, 'suitcase': 28, 'frisbee': 29, 'skis': 30, 'snowboard': 31, 'sports-ball': 32, 'kite': 33, 'baseball-bat': 34, 'baseball-glove': 35, 'skateboard': 36, 'surfboard': 37, 'tennis-racket': 38, 'bottle': 39, 'wine-glass': 40, 'cup': 41, 'fork': 42, 'knife': 43, 'spoon': 44, 'bowl': 45, 'banana': 46, 'apple': 47, 'sandwich': 48, 'orange': 49, 'broccoli': 50, 'carrot': 51, 'hot-dog': 52, 'pizza': 53, 'donut': 54, 'cake': 55, 'chair': 56, 'couch': 57, 'potted-plant': 58, 'bed': 59, 'dining-table': 60, 'toilet': 61, 'tv': 62, 'laptop': 63, 'mouse': 64, 'remote': 65, 'keyboard': 66, 'cell-phone': 67, 'microwave': 68, 'oven': 69, 'toaster': 70, 'sink': 71, 'refrigerator': 72, 'book': 73, 'clock': 74, 'vase': 75, 'scissors': 76, 'teddy-bear': 77, 'hair-drier': 78, 'toothbrush': 79}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_obj = {}\n",
        "for clas, index in obj_to_index.items():\n",
        "  index_to_obj[index] = clas\n",
        "\n",
        "print(len(index_to_obj))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_OqnKjvwdXC",
        "outputId": "95a11d9a-6197-4345-829f-39026987922b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(mod_path + '/img_to_objects.pkl', 'rb') as f:\n",
        "  img_to_desc = pickle.load(f)"
      ],
      "metadata": {
        "id": "UxHmTTjOxS5c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_to_inter_desc = {}\n",
        "for doc in img_to_desc.keys():\n",
        "  desc = img_to_desc[doc].split('\\n')[0].split(' ')[3:]\n",
        "  img_to_inter_desc[doc] = desc\n",
        "  print(desc)\n",
        "with open(mod_path + '/inter_dict.pkl', 'wb') as f:\n",
        "  pickle.dump(img_to_inter_desc, f)\n",
        "print(len(img_to_inter_desc))\n",
        "  "
      ],
      "metadata": {
        "id": "ArVrIfJFjkHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_obj_vector(inter_list):\n",
        "  n = len(inter_list)\n",
        "  obj_vector = np.zeros((80,1))\n",
        "  has_no_class = False\n",
        "\n",
        "  if inter_list[0] == '(no':\n",
        "      return obj_vector\n",
        "\n",
        "  for i in range(n):\n",
        "    if inter_list[i][-1] == ',':\n",
        "      inter_list[i] = inter_list[i][:-1]\n",
        "\n",
        "  if has_no_class:\n",
        "    return obj_vector\n",
        "    \n",
        "  count = 0\n",
        "\n",
        "  for i in range(0, n):\n",
        "\n",
        "    if inter_list[i] in obj_to_index:\n",
        "      obj_vector[obj_to_index[inter_list[i]]] = 1\n",
        "      count+=1\n",
        "    \n",
        "    elif inter_list[i][:-1] in obj_to_index:\n",
        "      obj_vector[obj_to_index[inter_list[i][:-1]]] = 1\n",
        "      count+=1\n",
        "    \n",
        "    elif i<n-1:\n",
        "      if (inter_list[i] + '-' + inter_list[i+1]) in obj_to_index:\n",
        "        obj_vector[obj_to_index[(inter_list[i] + '-' + inter_list[i+1])]] = 1\n",
        "        count+=1\n",
        "      elif (inter_list[i] + '-' + inter_list[i+1][:-1]) in obj_to_index:\n",
        "        obj_vector[obj_to_index[(inter_list[i] + '-' + inter_list[i+1][:-1])]] = 1\n",
        "        count+=1\n",
        "  return obj_vector\n"
      ],
      "metadata": {
        "id": "QpZZwTdPubqP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_to_object_vector = {}\n",
        "idx = 0\n",
        "for img, inter_list in img_to_inter_desc.items():\n",
        "  obj_vector = get_obj_vector(inter_list)\n",
        "  img_to_object_vector[img] = obj_vector\n",
        "  # print(obj_vector.shape)\n",
        "\n",
        "with open(mod_path+'/img_to_obj_vector_dict.pkl', 'wb') as f:\n",
        "  pickle.dump(img_to_object_vector, f)\n"
      ],
      "metadata": {
        "id": "kicoB5gpx3JY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(img_to_object_vector))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZVsr9XvwSXI",
        "outputId": "5cb83afe-6687-4c26-884a-0192be4b3325"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENDOF OBJ DETECTION**\n",
        "\n"
      ],
      "metadata": {
        "id": "aQ7h5DFhFVPU"
      }
    }
  ]
}